{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85732c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "# from statsmodels.base import elastic_net as els # 这个不会用。。用sklearn做弹性网\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import seaborn as sbn\n",
    "import researchpy as rp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['simhei']\n",
    "plt.rcParams['axes.unicode_minus']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5c2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义绘图方法\n",
    "def draw_freq(df, col_name, save_path=\"./image/\"):\n",
    "    ## 计数\n",
    "    y = df[col_name].value_counts()\n",
    "    x = y.index\n",
    "    \n",
    "    ## 绘制柱状图\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.bar(x,y, width=0.5)\n",
    "    for a, b in zip(x, y):\n",
    "        plt.text(a, b + 0.1, '%.0f' % b, ha='center', va='bottom', fontsize=15)\n",
    "    plt.title(\"Histogram Frequency Distribution of Variable \"+col_name, fontsize=10)\n",
    "#     plt.legend(handles=[line1, line2, line3, line4, line5], labels=[\"With\",\"Without\",\"Complete\",\"Traditional\", \"Non\"],fontsize=30)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.ylabel(\"Freq\",fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.grid(linestyle=\"--\", alpha=0.3)\n",
    "    plt.savefig(save_path+col_name+'_bar.png',dpi=600)\n",
    "    plt.close()\n",
    "    \n",
    "    ## 绘制饼图\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.pie(y,labels=x,autopct='%d%%')\n",
    "    plt.title(\"Pie Chart of Frequency Distribution of Variable \"+col_name, fontsize=10)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.savefig(save_path+col_name+'_pie.png',dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "def draw_hist(df, col_name, save_path=\"./image/\"):\n",
    "    # 首先计算极差，根据极差改柱体数量。本实验只有缺勤指标较大，因此直接输出就行\n",
    "    col_range = df[col_name].max()-df[col_name].min()\n",
    "    bins = col_range+1\n",
    "    ## 直方图\n",
    "    fig,ax = plt.subplots()\n",
    "    n,bins_num,pat = ax.hist(df[col_name], bins, (df[col_name].min()-0.5, df[col_name].max()+0.5))\n",
    "    ax.plot([i+0.5 for i in bins_num[:-1]], n)\n",
    "    \n",
    "    plt.title(\"Histogram Frequency Distribution of Variable \"+col_name, fontsize=10)\n",
    "    plt.ylabel(\"Freq\",fontsize=10)\n",
    "    plt.xlabel(col_name,fontsize=10)\n",
    "    plt.savefig(save_path+col_name+'_hist.png',dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdb72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读入数据\n",
    "df_raw = pd.read_csv(\"./data/student-mat.csv\", sep=\";\", header=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a095795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中缺失值的个数为：\n",
      " school        0\n",
      "sex           0\n",
      "age           0\n",
      "address       0\n",
      "famsize       0\n",
      "Pstatus       0\n",
      "Medu          0\n",
      "Fedu          0\n",
      "Mjob          0\n",
      "Fjob          0\n",
      "reason        0\n",
      "guardian      0\n",
      "traveltime    0\n",
      "studytime     0\n",
      "failures      0\n",
      "schoolsup     0\n",
      "famsup        0\n",
      "paid          0\n",
      "activities    0\n",
      "nursery       0\n",
      "higher        0\n",
      "internet      0\n",
      "romantic      0\n",
      "famrel        0\n",
      "freetime      0\n",
      "goout         0\n",
      "Dalc          0\n",
      "Walc          0\n",
      "health        0\n",
      "absences      0\n",
      "G1            0\n",
      "G2            0\n",
      "G3            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## 检查数据\n",
    "### 检查是否有缺失值\n",
    "print(\"数据中缺失值的个数为：\\n\", df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fc042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 实验报告废话生成：\n",
    "def feihua_out_1(df):\n",
    "    \n",
    "    file = open('./out.txt','w')\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        str_initial = \"学生 ({})的统计量的极差为{:.0f}，范围为{:.0f}-{:.0f}，中位数为{:.0f}，均值（方差，标准差）为{:.2f}({:.2f}, {:.2f})，峰度(Kurt)为{:.2f}，偏度(Skew)为{:.2f}。\".format(df[\"Variable\"][i],df[\"Range\"][i],df[\"min\"][i],df[\"max\"][i],df[\"50%\"][i],df[\"Mean\"][i],df[\"SD\"][i],df[\"SE\"][i],df[\"Kurtosis\"][i],df[\"Skew\"][i])\n",
    "        \n",
    "        str_0_0 = \"峰度小于0，说明数据较为分散；偏度小于0，说明数据整体呈左偏分布。\"\n",
    "        str_0_1 = \"峰度小于0，说明数据较为分散；偏度大于0，说明数据整体呈右偏分布。\"\n",
    "        str_1_0 = \"峰度大于0，说明数据较为集中；偏度小于0，说明数据整体呈左偏分布。\"\n",
    "        str_1_1 = \"峰度大于0，说明数据较为集中；偏度大于0，说明数据整体呈右偏分布。\"\n",
    "        str_out = \"\"\n",
    "        if df[\"Kurtosis\"][i]<0:\n",
    "            if df[\"Skew\"][i]<0:\n",
    "                str_out = str_initial + str_0_0\n",
    "            else:\n",
    "                str_out = str_initial + str_0_1\n",
    "        else:\n",
    "            if df[\"Skew\"][i]<0:\n",
    "                str_out = str_initial + str_1_0\n",
    "            else:\n",
    "                str_out = str_initial + str_1_1\n",
    "        print(str_out,file=file)\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "def feihua_out_2(df,col_name):\n",
    "    file = open('./out2.txt','a')\n",
    "    class_list = list(df.index)[:-1]\n",
    "    n_classes = len(class_list)\n",
    "    min_name = \"\"\n",
    "    max_name = \"\"\n",
    "    min_freq = 395\n",
    "    max_freq = 0\n",
    "    \n",
    "    str_initial = \"学生{}的统计量为名义变量，共有{:.0f}类，其中\".format(col_name, n_classes)\n",
    "    for i in range(n_classes):\n",
    "        if df[col_name][i] >= max_freq:\n",
    "            max_freq = df[col_name][i]\n",
    "            max_name = class_list[i]\n",
    "        if df[col_name][i] <= min_freq:\n",
    "            min_freq = df[col_name][i]\n",
    "            min_name = class_list[i]\n",
    "        add_str = \"类别({})的频数为{:.0f}，占比{:.2%},\".format(class_list[i], df[col_name][i], df[\"proportion(%)\"][i]/100)\n",
    "        str_initial += add_str\n",
    "    str_end = \"由此可见，该变量中，类别为{}的学生人数占比最多(频数为{:.0f})，类别为{}的学生人数占比最少(频数为{:.0f})。\" .format(max_name, max_freq, min_name, min_freq)\n",
    "    str_out = str_initial + str_end\n",
    "    print(str_out,file=file)\n",
    "    file.close()\n",
    "\n",
    "def feihua_out_3(df, col_name):\n",
    "    \n",
    "    file = open('./out3.txt','a')\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        str_initial = \"学生 ({})分类为{}的人数为{:.0f}，统计量的极差为{:.0f}，范围为{:.0f}-{:.0f}，中位数为{:.0f}，均值（标准差）为{:.2f}({:.2f})，峰度(Kurt)为{:.2f}，偏度(Skew)为{:.2f}。\".format(col_name,df[col_name][i],df[\"count\"][i],df[\"Range\"][i],df[\"min\"][i],df[\"max\"][i],df[\"50%\"][i],df[\"mean\"][i],df[\"std\"][i],df[\"Kurtosis\"][i],df[\"Skew\"][i])\n",
    "        \n",
    "        str_0_0 = \"峰度小于0，说明数据较为分散；偏度小于0，说明数据整体呈左偏分布。\"\n",
    "        str_0_1 = \"峰度小于0，说明数据较为分散；偏度大于0，说明数据整体呈右偏分布。\"\n",
    "        str_1_0 = \"峰度大于0，说明数据较为集中；偏度小于0，说明数据整体呈左偏分布。\"\n",
    "        str_1_1 = \"峰度大于0，说明数据较为集中；偏度大于0，说明数据整体呈右偏分布。\"\n",
    "        str_out = \"\"\n",
    "        if df[\"Kurtosis\"][i]<0:\n",
    "            if df[\"Skew\"][i]<0:\n",
    "                str_out = str_initial + str_0_0\n",
    "            else:\n",
    "                str_out = str_initial + str_0_1\n",
    "        else:\n",
    "            if df[\"Skew\"][i]<0:\n",
    "                str_out = str_initial + str_1_0\n",
    "            else:\n",
    "                str_out = str_initial + str_1_1\n",
    "        print(str_out,file=file)\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "def feihua_out_4(df, col_name):\n",
    "    file = open('./out4.txt','a')\n",
    "    \n",
    "    str_out = \"给定原假设H_0为：不同{}下因变量无差异；备择假设为：不同{}下因变量有差异。\\n经过方差分析检验(ANOVA)，在给定显著水平 {}=0.05的情况下，p值为{}，\".format(col_name,col_name,chr(945),df[\"PR(>F)\"][0])\n",
    "    str_0 = \" 大于{}，表示在原假设为真的情况下，统计量发生的概率较高，因此接受原假设，说明{}与因变量的相关性不显著。\".format(chr(945),col_name)\n",
    "    str_1 = \" 大于{}，表示在原假设为真的情况下，统计量发生的概率较低，因此拒绝原假设，说明{}与因变量的相关性较显著。\".format(chr(945),col_name)\n",
    "    if df[\"PR(>F)\"][0]>=0.05:\n",
    "        str_out += str_0\n",
    "    else:\n",
    "        str_out += str_1\n",
    "    print(str_out,file=file)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011fa1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数值变量包括： ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
      "\n",
      "\n",
      "名义变量包括： ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n"
     ]
    }
   ],
   "source": [
    "## 描述性统计\n",
    "### 数值型（包括数值变量，数值型分类变量）\n",
    "col_names_numeric = list(df_raw.describe().columns)\n",
    "print(\"数值变量包括：\",col_names_numeric)\n",
    "df_basic_describe = df_raw[col_names_numeric].describe().T\n",
    "\n",
    "df_describe_numeric = rp.summary_cont(df_raw[col_names_numeric])\n",
    "#### 偏度和峰度计算\n",
    "list_kurtosis, list_skew = [],[]\n",
    "for col_name in col_names_numeric:\n",
    "    list_kurtosis.append(df_raw[col_name].kurtosis())\n",
    "    list_skew.append(df_raw[col_name].skew())\n",
    "#     draw_hist(df_raw,col_name)\n",
    "df_describe_numeric[\"Kurtosis\"],df_describe_numeric[\"Skew\"]=list_kurtosis, list_skew\n",
    "df_describe_numeric.index = pd.Series(col_names_numeric)\n",
    "df_describe_numeric = pd.concat([df_basic_describe[[\"min\",\"25%\",\"50%\",\"75%\",\"max\"]],df_describe_numeric],axis=1)\n",
    "df_describe_numeric[\"Range\"] = df_basic_describe[\"max\"]-df_basic_describe[\"min\"]\n",
    "df_describe_numeric.to_excel(\"./file/describe.xlsx\")\n",
    "\n",
    "### 名义变量\n",
    "col_names_nominal = [i for i in list(df_raw.columns) if i not in col_names_numeric]\n",
    "print(\"名义变量包括：\",col_names_nominal)\n",
    "writer = pd.ExcelWriter(\"./file/value_counts.xlsx\")\n",
    "for col_name in col_names_nominal:\n",
    "    df_value_count = pd.DataFrame(df_raw[col_name].value_counts())\n",
    "    total_freq = df_value_count[col_name].sum()\n",
    "    df_value_count[\"proportion(%)\"] = df_value_count[col_name]/total_freq*100\n",
    "    df_value_count.loc[\"Total\"]=[total_freq,100]\n",
    "    df_value_count.to_excel(writer,sheet_name=col_name)\n",
    "#     draw_freq(df_raw, col_name)\n",
    "#     feihua_out_2(df_value_count,col_name)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e6e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feihua_out_1(df_describe_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d887d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义分组绘图的方法\n",
    "def draw_group_by_nominal(df, IV_name, DV_name, save_path=\"./image/\"):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    df_each = df[[IV_name, DV_name]]\n",
    "    fig = sbn.boxplot(x=IV_name, y=DV_name, data=df_each)\n",
    "    plt.title(\"Boxplot of Variable \"+DV_name+\" group by \"+IV_name, fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.ylabel(DV_name,fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.xlabel(IV_name,fontsize=20)\n",
    "    plt.grid(linestyle=\"--\", alpha=0.3)\n",
    "    plt.savefig(save_path+IV_name+'_'+DV_name+'_box_plot.png',dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "def draw_group_by_numeric(df, IV_name, DV_name, save_path=\"./image/\"):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    df_each = df[[IV_name, DV_name]]\n",
    "    fig = (sbn.jointplot(x=IV_name, y=DV_name, data=df_each, marginal_ticks=True, kind='reg',color=\"orange\", scatter_kws={'s': [5]}).plot_joint(sbn.kdeplot, color='royalblue'))\n",
    "#     plt.title(\"Plot of Variable \"+DV_name+\" and \"+IV_name, fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.ylabel(DV_name,fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.xlabel(IV_name,fontsize=20)\n",
    "    plt.grid(linestyle=\"--\", alpha=0.3)\n",
    "    fig.savefig(save_path+IV_name+'_'+DV_name+'_scatter_plot.png',dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "def draw_heat_map(df, save_path=\"./image/\"):\n",
    "    ## this df is the cor\n",
    "    plt.figure(figsize=(12,12))\n",
    "    fig = sbn.heatmap(data = df_numeric.corr(),annot = True, fmt=\".2f\", vmin=-1.0, vmax=1.0, cmap=\"RdBu_r\")\n",
    "    fig.set(xlabel=\"\", ylabel=\"\")\n",
    "    fig.xaxis.tick_top()\n",
    "    plt.title(\"Plot of Cor\", fontsize=20)\n",
    "    plt.savefig(save_path+'heat_map_cor.png',dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "375080d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_anova(df, DV_name, IV_name, writer):\n",
    "    \n",
    "    df_each = df[[IV_name,DV_name]]\n",
    "    model_each = ols('G3~C('+IV_name+')', data=df_each).fit()\n",
    "    anova_test = anova_lm(model_each)\n",
    "    anova_test.to_excel(writer,sheet_name=IV_name)\n",
    "    \n",
    "    return anova_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c9c5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11268\\1187730233.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 与G3正相关的有：\n",
      " Medu,Fedu,studytime,famrel,freetime,absences,G1,G2,G3\n",
      " 与G3负相关的有：\n",
      " age,traveltime,failures,goout,Dalc,Walc,health\n"
     ]
    }
   ],
   "source": [
    "## 分组的因变量描述性统计 （因变量为G3，因此只对G3做分组描述统计）\n",
    "### 分类变量与G3\n",
    "writer = pd.ExcelWriter(\"./file/groupby_describe.xlsx\")\n",
    "for col_name in col_names_nominal:\n",
    "    df_each = df_raw[[col_name,'G3']].groupby([col_name]).describe()\n",
    "    df_each = df_each.droplevel(0,axis=1)\n",
    "    df_each.index.name=\"\"   \n",
    "    df_each.insert(loc=0, column=col_name, value=list(df_each.index))\n",
    "    \n",
    "    df_each[\"Skew\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).skew()[\"G3\"])\n",
    "    df_each[\"Kurtosis\"] = list(df_raw[[col_name,'G3']].groupby([col_name]).apply(pd.DataFrame.kurt)[\"G3\"])\n",
    "    df_each[\"Range\"] = df_each[(\"max\")] - df_each[(\"min\")]\n",
    "    df_each.to_excel(writer,sheet_name=\"G_by_\"+col_name, index=False)\n",
    "\n",
    "    feihua_out_3(df_each, col_name)\n",
    "#     draw_group_by_nominal(df_raw, col_name, \"G3\")\n",
    "\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "writer = pd.ExcelWriter(\"./file/anova_nominal.xlsx\")\n",
    "for col_name in col_names_nominal:\n",
    "    df_anova = my_anova(df_raw, \"G3\", col_name, writer)\n",
    "    feihua_out_4(df_anova, col_name)\n",
    "\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "### 数值变量与G3\n",
    "df_numeric = df_raw[col_names_numeric]\n",
    "#### 相关系数检验\n",
    "df_numeric.corr().to_excel(\"./file/correlation.xlsx\")\n",
    "#### 相关系数热力图\n",
    "draw_heat_map(df_numeric.corr())\n",
    "#### 相关性绝对值高低判断\n",
    "print(\" 与G3正相关的有：\\n\", \",\".join(list(df_numeric.corr()[\"G3\"].loc[df_numeric.corr()[\"G3\"]>=0].index)))\n",
    "print(\" 与G3负相关的有：\\n\", \",\".join(list(df_numeric.corr()[\"G3\"].loc[df_numeric.corr()[\"G3\"]<0].index)))\n",
    "#### 相关性高低\n",
    "# df_numeric.corr()[\"G3\"].apply(abs).sort_values(ascending=False).to_excel(\"./file/cor_rank.xlsx\")\n",
    "\n",
    "\n",
    "#### 挨个画图 会报错，但是能成功保存图片，具体原因未知。\n",
    "# for col_name in col_names_numeric:\n",
    "#     draw_group_by_numeric(df_raw, col_name, \"G3\")\n",
    "\n",
    "#### 绘制图 已绘制\n",
    "# sbn.set_theme(style=\"ticks\")\n",
    "\n",
    "# fig = sbn.pairplot(df_numeric,kind=\"reg\",diag_kind=\"kde\")\n",
    "# fig.savefig(\"./image/pairplot_total_1.png\", dpi = 600)\n",
    "\n",
    "# fig = sbn.pairplot(df_numeric,kind=\"reg\",diag_kind=\"hist\")\n",
    "# fig.savefig(\"./image/pairplot_total_2.png\", dpi = 600)\n",
    "\n",
    "# fig = sbn.pairplot(df_numeric,x_vars=col_names_numeric, y_vars=[\"G3\"], kind=\"reg\",diag_kind=\"hist\")\n",
    "# fig.savefig(\"./image/pairplot_G3.png\", dpi = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6638b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_OLS(df,IV_names,DV_name, with_const = False, random_del = False, random_seed = None):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    new_IV_names = IV_names.copy()\n",
    "    if with_const:\n",
    "        LR_full = sm.OLS(df[DV_names], sm.add_constant(df[IV_names])).fit()\n",
    "    else: \n",
    "        LR_full = sm.OLS(df[DV_names], df[IV_names]).fit()\n",
    "    \n",
    "    df_pvalues = LR_full.pvalues\n",
    "    del_variable_list = list(df_pvalues.loc[df_pvalues>0.05].index)\n",
    "    if 'const' in del_variable_list:\n",
    "        del_variable_list.remove('const')\n",
    "    \n",
    "    if random_del:\n",
    "        np.random.shuffle(del_variable_list)\n",
    "    \n",
    "    LR_step_list = [LR_full]\n",
    "    while True:\n",
    "        if len(del_variable_list)>0:\n",
    "            new_IV_names.remove(del_variable_list[0])\n",
    "        else:\n",
    "            break\n",
    "        LR_step = sm.OLS(df[DV_names], df[new_IV_names]).fit()\n",
    "        \n",
    "        df_pvalues = LR_step.pvalues\n",
    "        del_variable_list = list(df_pvalues.loc[df_pvalues>0.05].index)\n",
    "        if 'const' in del_variable_list:\n",
    "            del_variable_list.remove('const')\n",
    "        if random_del:\n",
    "            np.random.shuffle(del_variable_list)\n",
    "        \n",
    "        LR_step_list.append(LR_step)\n",
    "    \n",
    "    return LR_step_list\n",
    "\n",
    "def step_OLS_to_excel(df, IV_names, DV_name, file_path, with_const = False, random_del = False, random_seed = None):\n",
    "    LR_step_list = step_OLS(df, IV_names, DV_name, with_const, random_del, random_seed)\n",
    "    writer = pd.ExcelWriter(file_path)\n",
    "    for step, i_summary in enumerate([i.summary() for i in LR_step_list]):\n",
    "        df_table_1 =  pd.read_html(i_summary.tables[0].as_html())[0]\n",
    "\n",
    "        df_table_2 =  pd.read_html(i_summary.tables[1].as_html())[0]\n",
    "        df_table_2.columns = np.array(df_table_2).tolist()[0]\n",
    "        df_table_2.drop([0], inplace=True)\n",
    "        df_table_2 = df_table_2.reset_index(drop=True)\n",
    "\n",
    "        df_table_3 =  pd.read_html(i_summary.tables[2].as_html())[0]\n",
    "        pd.concat([df_table_1, df_table_2, df_table_3], axis=1).to_excel(writer,sheet_name=\"Step_\"+str(step), index=False) \n",
    "\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    return LR_step_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95514ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多元线性回归\n",
    "### 使用one-hot对分类变量做处理\n",
    "df_produce = df_raw.copy()\n",
    "df_produce = df_produce.drop(columns=[\"G1\",\"G2\"])\n",
    "\n",
    "for col_name in col_names_nominal:\n",
    "    df_each = pd.get_dummies(df_produce[col_name],prefix=col_name,drop_first=True)\n",
    "#     df_each.columns = [col_name+\".\"+i for i in list(df_each.columns)] 等价于prefix\n",
    "    df_produce = df_produce.drop(columns=[col_name])\n",
    "    df_produce = pd.concat([df_produce,df_each],axis=1)\n",
    "df_produce.to_excel(\"./data/data_produce.xlsx\", index=False)\n",
    "### 全部变量一起做回归\n",
    "IV_names = list(df_produce.columns)\n",
    "IV_names.remove(\"G3\")\n",
    "DV_names = \"G3\"\n",
    "\n",
    "## 带截距项\n",
    "LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_sequence_const.xlsx\", with_const=True, random_del=False)\n",
    "\n",
    "\n",
    "## 不带截距项\n",
    "\n",
    "### step regression\n",
    "#### random_del=False 默认表示按顺序删变量，否则随机删除变量。random_seed表示numpy随机数种子，默认值为None\n",
    "LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_sequence.xlsx\", random_del=False)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed.xlsx\", random_del=True)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed_0.xlsx\", random_del=True, random_seed=0)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed_1.xlsx\", random_del=True, random_seed=1)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed_2.xlsx\", random_del=True, random_seed=2)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed_3.xlsx\", random_del=True, random_seed=3)\n",
    "# LR_step_list = step_OLS_to_excel(df_produce,IV_names,DV_names,\"./file/OLS_by_step_seed_4.xlsx\", random_del=True, random_seed=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d8047ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                     G3   R-squared (uncentered):                   0.880\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.866\n",
      "Method:                 Least Squares   F-statistic:                              66.65\n",
      "Date:                Sat, 29 Oct 2022   Prob (F-statistic):                   9.48e-140\n",
      "Time:                        18:30:31   Log-Likelihood:                         -1102.9\n",
      "No. Observations:                 395   AIC:                                      2284.\n",
      "Df Residuals:                     356   BIC:                                      2439.\n",
      "Df Model:                          39                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "age                   0.1985      0.119      1.666      0.097      -0.036       0.433\n",
      "Medu                  0.5331      0.326      1.634      0.103      -0.109       1.175\n",
      "Fedu                 -0.0079      0.279     -0.028      0.977      -0.557       0.541\n",
      "traveltime            0.0009      0.334      0.003      0.998      -0.656       0.658\n",
      "studytime             0.6252      0.290      2.154      0.032       0.054       1.196\n",
      "failures             -1.6615      0.336     -4.939      0.000      -2.323      -1.000\n",
      "famrel                0.2759      0.249      1.110      0.268      -0.213       0.765\n",
      "freetime              0.4509      0.235      1.915      0.056      -0.012       0.914\n",
      "goout                -0.6715      0.226     -2.973      0.003      -1.116      -0.227\n",
      "Dalc                 -0.3193      0.335     -0.954      0.341      -0.977       0.339\n",
      "Walc                  0.2872      0.251      1.144      0.253      -0.206       0.781\n",
      "health               -0.0962      0.161     -0.598      0.551      -0.413       0.220\n",
      "absences              0.0461      0.029      1.581      0.115      -0.011       0.103\n",
      "school_MS             0.1544      0.780      0.198      0.843      -1.379       1.688\n",
      "sex_M                 1.3448      0.506      2.660      0.008       0.351       2.339\n",
      "address_U             0.8869      0.581      1.526      0.128      -0.256       2.030\n",
      "famsize_LE3           0.7745      0.494      1.569      0.118      -0.197       1.746\n",
      "Pstatus_T            -0.0618      0.728     -0.085      0.932      -1.494       1.370\n",
      "Mjob_health           0.8887      1.131      0.785      0.433      -1.337       3.114\n",
      "Mjob_other           -0.2901      0.722     -0.402      0.688      -1.709       1.129\n",
      "Mjob_services         0.5714      0.807      0.708      0.480      -1.016       2.159\n",
      "Mjob_teacher         -1.4722      1.048     -1.404      0.161      -3.534       0.590\n",
      "Fjob_health           1.2332      1.428      0.864      0.388      -1.574       4.041\n",
      "Fjob_other            0.1189      1.008      0.118      0.906      -1.863       2.101\n",
      "Fjob_services         0.3668      1.036      0.354      0.723      -1.670       2.404\n",
      "Fjob_teacher          1.9909      1.295      1.537      0.125      -0.556       4.538\n",
      "reason_home           0.1940      0.559      0.347      0.729      -0.906       1.294\n",
      "reason_other          1.0341      0.824      1.256      0.210      -0.586       2.654\n",
      "reason_reputation     0.6849      0.583      1.174      0.241      -0.462       1.832\n",
      "guardian_mother       0.3012      0.547      0.550      0.582      -0.775       1.378\n",
      "guardian_other        0.0204      0.984      0.021      0.983      -1.915       1.956\n",
      "schoolsup_yes        -0.9229      0.661     -1.396      0.163      -2.223       0.377\n",
      "famsup_yes           -0.7536      0.483     -1.559      0.120      -1.704       0.197\n",
      "paid_yes              0.2608      0.483      0.540      0.590      -0.689       1.211\n",
      "activities_yes       -0.3230      0.450     -0.717      0.474      -1.209       0.563\n",
      "nursery_yes          -0.0709      0.555     -0.128      0.898      -1.162       1.021\n",
      "higher_yes            2.5816      1.019      2.534      0.012       0.578       4.585\n",
      "internet_yes          0.7575      0.622      1.219      0.224      -0.465       1.980\n",
      "romantic_yes         -1.1559      0.475     -2.435      0.015      -2.089      -0.222\n",
      "==============================================================================\n",
      "Omnibus:                       31.090   Durbin-Watson:                   2.026\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.184\n",
      "Skew:                          -0.702   Prob(JB):                     1.39e-08\n",
      "Kurtosis:                       3.476   Cond. No.                         202.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "### full regression\n",
    "print(LR_step_list[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c216871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                     G3   R-squared (uncentered):                   0.858\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.856\n",
      "Method:                 Least Squares   F-statistic:                              335.8\n",
      "Date:                Sun, 30 Oct 2022   Prob (F-statistic):                   2.43e-160\n",
      "Time:                        10:37:40   Log-Likelihood:                         -1134.9\n",
      "No. Observations:                 395   AIC:                                      2284.\n",
      "Df Residuals:                     388   BIC:                                      2312.\n",
      "Df Model:                           7                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "studytime        0.8873      0.258      3.441      0.001       0.380       1.394\n",
      "failures        -1.5537      0.297     -5.234      0.000      -2.137      -0.970\n",
      "freetime         0.5183      0.202      2.570      0.011       0.122       0.915\n",
      "sex_M            1.8078      0.460      3.930      0.000       0.903       2.712\n",
      "reason_other     1.6453      0.756      2.177      0.030       0.159       3.131\n",
      "higher_yes       5.3284      0.777      6.854      0.000       3.800       6.857\n",
      "internet_yes     1.5008      0.565      2.658      0.008       0.391       2.611\n",
      "==============================================================================\n",
      "Omnibus:                       24.693   Durbin-Watson:                   2.054\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               27.617\n",
      "Skew:                          -0.607   Prob(JB):                     1.01e-06\n",
      "Kurtosis:                       3.450   Cond. No.                         15.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "### final regression\n",
    "print(LR_step_list[-1].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "074b5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a question: different sequence may have different result. Use np.random.shuffle(<list>) to random the col_name\n",
    "### 结果表明先后顺序会影响，因此最终结果是个局部最优\n",
    "\n",
    "### l1_ratio表示l1正则（r）和l2（1-r）正则的程度\n",
    "elastic_model = lm.ElasticNet(l1_ratio = 1)\n",
    "elastic_result = elastic_model.fit(df_produce[IV_names], df_produce[DV_names])\n",
    "df_elastic_result = pd.DataFrame()\n",
    "df_elastic_result['Variable'] = IV_names\n",
    "df_elastic_result['coef'] = elastic_result.coef_\n",
    "df_elastic_result.to_excel('./file/elastic_ls_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06910b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debb5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   R-squared:                       0.276\n",
      "Model:                            OLS   Adj. R-squared:                  0.196\n",
      "Method:                 Least Squares   F-statistic:                     3.463\n",
      "Date:                Sun, 30 Oct 2022   Prob (F-statistic):           3.32e-10\n",
      "Time:                        16:04:27   Log-Likelihood:                -1097.5\n",
      "No. Observations:                 395   AIC:                             2275.\n",
      "Df Residuals:                     355   BIC:                             2434.\n",
      "Df Model:                          39                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                14.0777      4.481      3.142      0.002       5.265      22.890\n",
      "age                  -0.3752      0.217     -1.727      0.085      -0.802       0.052\n",
      "Medu                  0.4569      0.323      1.414      0.158      -0.179       1.092\n",
      "Fedu                 -0.1046      0.278     -0.377      0.707      -0.651       0.441\n",
      "traveltime           -0.2403      0.339     -0.709      0.479      -0.907       0.426\n",
      "studytime             0.5495      0.288      1.910      0.057      -0.016       1.115\n",
      "failures             -1.7240      0.333     -5.179      0.000      -2.379      -1.069\n",
      "famrel                0.2316      0.246      0.942      0.347      -0.252       0.715\n",
      "freetime              0.3024      0.237      1.274      0.203      -0.164       0.769\n",
      "goout                -0.5937      0.225     -2.644      0.009      -1.035      -0.152\n",
      "Dalc                 -0.2722      0.331     -0.823      0.411      -0.923       0.378\n",
      "Walc                  0.2634      0.248      1.062      0.289      -0.224       0.751\n",
      "health               -0.1768      0.161     -1.098      0.273      -0.493       0.140\n",
      "absences              0.0563      0.029      1.943      0.053      -0.001       0.113\n",
      "school_MS             0.7256      0.792      0.917      0.360      -0.831       2.282\n",
      "sex_M                 1.2624      0.500      2.525      0.012       0.279       2.246\n",
      "address_U             0.5513      0.584      0.944      0.346      -0.597       1.700\n",
      "famsize_LE3           0.7028      0.488      1.439      0.151      -0.257       1.663\n",
      "Pstatus_T            -0.3201      0.724     -0.442      0.659      -1.744       1.104\n",
      "Mjob_health           0.9981      1.118      0.893      0.373      -1.201       3.197\n",
      "Mjob_other           -0.3590      0.713     -0.503      0.615      -1.762       1.044\n",
      "Mjob_services         0.6583      0.798      0.825      0.410      -0.911       2.227\n",
      "Mjob_teacher         -1.2415      1.038     -1.196      0.233      -3.283       0.800\n",
      "Fjob_health           0.3477      1.438      0.242      0.809      -2.480       3.176\n",
      "Fjob_other           -0.6197      1.023     -0.606      0.545      -2.632       1.392\n",
      "Fjob_services        -0.4658      1.057     -0.441      0.660      -2.544       1.613\n",
      "Fjob_teacher          1.3262      1.297      1.023      0.307      -1.224       3.876\n",
      "reason_home           0.0785      0.554      0.142      0.887      -1.011       1.168\n",
      "reason_other          0.7771      0.818      0.950      0.343      -0.831       2.385\n",
      "reason_reputation     0.6130      0.577      1.063      0.288      -0.521       1.747\n",
      "guardian_mother       0.0698      0.546      0.128      0.898      -1.003       1.143\n",
      "guardian_other        0.7501      0.999      0.751      0.453      -1.216       2.716\n",
      "schoolsup_yes        -1.3506      0.667     -2.025      0.044      -2.662      -0.039\n",
      "famsup_yes           -0.8618      0.479     -1.800      0.073      -1.803       0.080\n",
      "paid_yes              0.3397      0.478      0.711      0.477      -0.600       1.279\n",
      "activities_yes       -0.3295      0.445     -0.741      0.459      -1.205       0.546\n",
      "nursery_yes          -0.1773      0.549     -0.323      0.747      -1.258       0.903\n",
      "higher_yes            1.3705      1.078      1.272      0.204      -0.749       3.490\n",
      "internet_yes          0.4981      0.620      0.804      0.422      -0.720       1.717\n",
      "romantic_yes         -1.0945      0.469     -2.332      0.020      -2.017      -0.172\n",
      "==============================================================================\n",
      "Omnibus:                       30.431   Durbin-Watson:                   2.054\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               35.239\n",
      "Skew:                          -0.696   Prob(JB):                     2.23e-08\n",
      "Kurtosis:                       3.450   Cond. No.                         443.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "test_OLS = OLS(df_produce[DV_names],sm.add_constant(df_produce[IV_names])).fit()\n",
    "print(test_OLS.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0a075ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   R-squared:                       0.276\n",
      "Model:                            OLS   Adj. R-squared:                  0.196\n",
      "Method:                 Least Squares   F-statistic:                     3.463\n",
      "Date:                Sun, 30 Oct 2022   Prob (F-statistic):           3.32e-10\n",
      "Time:                        11:09:40   Log-Likelihood:                -1097.5\n",
      "No. Observations:                 395   AIC:                             2275.\n",
      "Df Residuals:                     355   BIC:                             2434.\n",
      "Df Model:                          39                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "age                  -0.3752      0.217     -1.727      0.085      -0.802       0.052\n",
      "Medu                  0.4569      0.323      1.414      0.158      -0.179       1.092\n",
      "Fedu                 -0.1046      0.278     -0.377      0.707      -0.651       0.441\n",
      "traveltime           -0.2403      0.339     -0.709      0.479      -0.907       0.426\n",
      "studytime             0.5495      0.288      1.910      0.057      -0.016       1.115\n",
      "failures             -1.7240      0.333     -5.179      0.000      -2.379      -1.069\n",
      "famrel                0.2316      0.246      0.942      0.347      -0.252       0.715\n",
      "freetime              0.3024      0.237      1.274      0.203      -0.164       0.769\n",
      "goout                -0.5937      0.225     -2.644      0.009      -1.035      -0.152\n",
      "Dalc                 -0.2722      0.331     -0.823      0.411      -0.923       0.378\n",
      "Walc                  0.2634      0.248      1.062      0.289      -0.224       0.751\n",
      "health               -0.1768      0.161     -1.098      0.273      -0.493       0.140\n",
      "absences              0.0563      0.029      1.943      0.053      -0.001       0.113\n",
      "school_MS             0.7256      0.792      0.917      0.360      -0.831       2.282\n",
      "sex_M                 1.2624      0.500      2.525      0.012       0.279       2.246\n",
      "address_U             0.5513      0.584      0.944      0.346      -0.597       1.700\n",
      "famsize_LE3           0.7028      0.488      1.439      0.151      -0.257       1.663\n",
      "Pstatus_T            -0.3201      0.724     -0.442      0.659      -1.744       1.104\n",
      "Mjob_health           0.9981      1.118      0.893      0.373      -1.201       3.197\n",
      "Mjob_other           -0.3590      0.713     -0.503      0.615      -1.762       1.044\n",
      "Mjob_services         0.6583      0.798      0.825      0.410      -0.911       2.227\n",
      "Mjob_teacher         -1.2415      1.038     -1.196      0.233      -3.283       0.800\n",
      "Fjob_health           0.3477      1.438      0.242      0.809      -2.480       3.176\n",
      "Fjob_other           -0.6197      1.023     -0.606      0.545      -2.632       1.392\n",
      "Fjob_services        -0.4658      1.057     -0.441      0.660      -2.544       1.613\n",
      "Fjob_teacher          1.3262      1.297      1.023      0.307      -1.224       3.876\n",
      "reason_home           0.0785      0.554      0.142      0.887      -1.011       1.168\n",
      "reason_other          0.7771      0.818      0.950      0.343      -0.831       2.385\n",
      "reason_reputation     0.6130      0.577      1.063      0.288      -0.521       1.747\n",
      "guardian_mother       0.0698      0.546      0.128      0.898      -1.003       1.143\n",
      "guardian_other        0.7501      0.999      0.751      0.453      -1.216       2.716\n",
      "schoolsup_yes        -1.3506      0.667     -2.025      0.044      -2.662      -0.039\n",
      "famsup_yes           -0.8618      0.479     -1.800      0.073      -1.803       0.080\n",
      "paid_yes              0.3397      0.478      0.711      0.477      -0.600       1.279\n",
      "activities_yes       -0.3295      0.445     -0.741      0.459      -1.205       0.546\n",
      "nursery_yes          -0.1773      0.549     -0.323      0.747      -1.258       0.903\n",
      "higher_yes            1.3705      1.078      1.272      0.204      -0.749       3.490\n",
      "internet_yes          0.4981      0.620      0.804      0.422      -0.720       1.717\n",
      "romantic_yes         -1.0945      0.469     -2.332      0.020      -2.017      -0.172\n",
      "0                    14.0777      4.481      3.142      0.002       5.265      22.890\n",
      "==============================================================================\n",
      "Omnibus:                       30.431   Durbin-Watson:                   2.054\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               35.239\n",
      "Skew:                          -0.696   Prob(JB):                     2.23e-08\n",
      "Kurtosis:                       3.450   Cond. No.                         443.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   R-squared:                       0.276\n",
      "Model:                            OLS   Adj. R-squared:                  0.196\n",
      "Method:                 Least Squares   F-statistic:                     3.463\n",
      "Date:                Sun, 30 Oct 2022   Prob (F-statistic):           3.32e-10\n",
      "Time:                        11:09:40   Log-Likelihood:                -1097.5\n",
      "No. Observations:                 395   AIC:                             2275.\n",
      "Df Residuals:                     355   BIC:                             2434.\n",
      "Df Model:                          39                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            14.0777      4.481      3.142      0.002       5.265      22.890\n",
      "age                  -0.3752      0.217     -1.727      0.085      -0.802       0.052\n",
      "Medu                  0.4569      0.323      1.414      0.158      -0.179       1.092\n",
      "Fedu                 -0.1046      0.278     -0.377      0.707      -0.651       0.441\n",
      "traveltime           -0.2403      0.339     -0.709      0.479      -0.907       0.426\n",
      "studytime             0.5495      0.288      1.910      0.057      -0.016       1.115\n",
      "failures             -1.7240      0.333     -5.179      0.000      -2.379      -1.069\n",
      "famrel                0.2316      0.246      0.942      0.347      -0.252       0.715\n",
      "freetime              0.3024      0.237      1.274      0.203      -0.164       0.769\n",
      "goout                -0.5937      0.225     -2.644      0.009      -1.035      -0.152\n",
      "Dalc                 -0.2722      0.331     -0.823      0.411      -0.923       0.378\n",
      "Walc                  0.2634      0.248      1.062      0.289      -0.224       0.751\n",
      "health               -0.1768      0.161     -1.098      0.273      -0.493       0.140\n",
      "absences              0.0563      0.029      1.943      0.053      -0.001       0.113\n",
      "school_MS             0.7256      0.792      0.917      0.360      -0.831       2.282\n",
      "sex_M                 1.2624      0.500      2.525      0.012       0.279       2.246\n",
      "address_U             0.5513      0.584      0.944      0.346      -0.597       1.700\n",
      "famsize_LE3           0.7028      0.488      1.439      0.151      -0.257       1.663\n",
      "Pstatus_T            -0.3201      0.724     -0.442      0.659      -1.744       1.104\n",
      "Mjob_health           0.9981      1.118      0.893      0.373      -1.201       3.197\n",
      "Mjob_other           -0.3590      0.713     -0.503      0.615      -1.762       1.044\n",
      "Mjob_services         0.6583      0.798      0.825      0.410      -0.911       2.227\n",
      "Mjob_teacher         -1.2415      1.038     -1.196      0.233      -3.283       0.800\n",
      "Fjob_health           0.3477      1.438      0.242      0.809      -2.480       3.176\n",
      "Fjob_other           -0.6197      1.023     -0.606      0.545      -2.632       1.392\n",
      "Fjob_services        -0.4658      1.057     -0.441      0.660      -2.544       1.613\n",
      "Fjob_teacher          1.3262      1.297      1.023      0.307      -1.224       3.876\n",
      "reason_home           0.0785      0.554      0.142      0.887      -1.011       1.168\n",
      "reason_other          0.7771      0.818      0.950      0.343      -0.831       2.385\n",
      "reason_reputation     0.6130      0.577      1.063      0.288      -0.521       1.747\n",
      "guardian_mother       0.0698      0.546      0.128      0.898      -1.003       1.143\n",
      "guardian_other        0.7501      0.999      0.751      0.453      -1.216       2.716\n",
      "schoolsup_yes        -1.3506      0.667     -2.025      0.044      -2.662      -0.039\n",
      "famsup_yes           -0.8618      0.479     -1.800      0.073      -1.803       0.080\n",
      "paid_yes              0.3397      0.478      0.711      0.477      -0.600       1.279\n",
      "activities_yes       -0.3295      0.445     -0.741      0.459      -1.205       0.546\n",
      "nursery_yes          -0.1773      0.549     -0.323      0.747      -1.258       0.903\n",
      "higher_yes            1.3705      1.078      1.272      0.204      -0.749       3.490\n",
      "internet_yes          0.4981      0.620      0.804      0.422      -0.720       1.717\n",
      "romantic_yes         -1.0945      0.469     -2.332      0.020      -2.017      -0.172\n",
      "==============================================================================\n",
      "Omnibus:                       30.431   Durbin-Watson:                   2.054\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               35.239\n",
      "Skew:                          -0.696   Prob(JB):                     2.23e-08\n",
      "Kurtosis:                       3.450   Cond. No.                         443.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "test_OLS = OLS(df_produce[DV_names],pd.concat([df_produce[IV_names],pd.DataFrame([1]*len(df_produce))],axis=1)).fit()\n",
    "print(test_OLS.summary())\n",
    "test_ols = ols('G3~'+'+'.join(IV_names), data=df_produce).fit()\n",
    "print(test_ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f3d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OLS in module statsmodels.regression.linear_model:\n",
      "\n",
      "class OLS(WLS)\n",
      " |  OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |  \n",
      " |  Ordinary Least Squares\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      A 1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  hasconst : None or bool\n",
      " |      Indicates whether the RHS includes a user-supplied constant. If True,\n",
      " |      a constant is not checked for and k_constant is set to 1 and all\n",
      " |      result statistics are calculated as if a constant is present. If\n",
      " |      False, a constant is not checked for and k_constant is set to 0.\n",
      " |  **kwargs\n",
      " |      Extra arguments that are used to set model properties when using the\n",
      " |      formula interface.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  weights : scalar\n",
      " |      Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  WLS : Fit a linear model using Weighted Least Squares.\n",
      " |  GLS : Fit a linear model using Generalized Least Squares.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  No constant is added by the model unless you are using formulas.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import statsmodels.api as sm\n",
      " |  >>> import numpy as np\n",
      " |  >>> duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
      " |  >>> Y = duncan_prestige.data['income']\n",
      " |  >>> X = duncan_prestige.data['education']\n",
      " |  >>> X = sm.add_constant(X)\n",
      " |  >>> model = sm.OLS(Y,X)\n",
      " |  >>> results = model.fit()\n",
      " |  >>> results.params\n",
      " |  const        10.603498\n",
      " |  education     0.594859\n",
      " |  dtype: float64\n",
      " |  \n",
      " |  >>> results.tvalues\n",
      " |  const        2.039813\n",
      " |  education    6.892802\n",
      " |  dtype: float64\n",
      " |  \n",
      " |  >>> print(results.t_test([1, 0]))\n",
      " |                               Test for Constraints\n",
      " |  ==============================================================================\n",
      " |                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      " |  ------------------------------------------------------------------------------\n",
      " |  c0            10.6035      5.198      2.040      0.048       0.120      21.087\n",
      " |  ==============================================================================\n",
      " |  \n",
      " |  >>> print(results.f_test(np.identity(2)))\n",
      " |  <F test: F=array([[159.63031026]]), p=1.2607168903696672e-20, df_denom=43, df_num=2>\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OLS\n",
      " |      WLS\n",
      " |      RegressionModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_regularized(self, method='elastic_net', alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False, **kwargs)\n",
      " |      Return a regularized fit to a linear regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str\n",
      " |          Either 'elastic_net' or 'sqrt_lasso'.\n",
      " |      alpha : scalar or array_like\n",
      " |          The penalty weight.  If a scalar, the same penalty weight\n",
      " |          applies to all variables in the model.  If a vector, it\n",
      " |          must have the same length as `params`, and contains a\n",
      " |          penalty weight for each coefficient.\n",
      " |      L1_wt : scalar\n",
      " |          The fraction of the penalty given to the L1 penalty term.\n",
      " |          Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
      " |          ridge fit, if 1 it is a lasso fit.\n",
      " |      start_params : array_like\n",
      " |          Starting values for ``params``.\n",
      " |      profile_scale : bool\n",
      " |          If True the penalized fit is computed using the profile\n",
      " |          (concentrated) log-likelihood for the Gaussian model.\n",
      " |          Otherwise the fit uses the residual sum of squares.\n",
      " |      refit : bool\n",
      " |          If True, the model is refit using only the variables that\n",
      " |          have non-zero coefficients in the regularized fit.  The\n",
      " |          refitted model is not regularized.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      statsmodels.base.elastic_net.RegularizedResults\n",
      " |          The regularized results.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The elastic net uses a combination of L1 and L2 penalties.\n",
      " |      The implementation closely follows the glmnet package in R.\n",
      " |      \n",
      " |      The function that is minimized is:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          0.5*RSS/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
      " |      \n",
      " |      where RSS is the usual regression sum of squares, n is the\n",
      " |      sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
      " |      norms.\n",
      " |      \n",
      " |      For WLS and GLS, the RSS is calculated using the whitened endog and\n",
      " |      exog data.\n",
      " |      \n",
      " |      Post-estimation results are based on the same data used to\n",
      " |      select variables, hence may be subject to overfitting biases.\n",
      " |      \n",
      " |      The elastic_net method uses the following keyword arguments:\n",
      " |      \n",
      " |      maxiter : int\n",
      " |          Maximum number of iterations\n",
      " |      cnvrg_tol : float\n",
      " |          Convergence threshold for line searches\n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The square root lasso approach is a variation of the Lasso\n",
      " |      that is largely self-tuning (the optimal tuning parameter\n",
      " |      does not depend on the standard deviation of the regression\n",
      " |      errors).  If the errors are Gaussian, the tuning parameter\n",
      " |      can be taken to be\n",
      " |      \n",
      " |      alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p))\n",
      " |      \n",
      " |      where n is the sample size and p is the number of predictors.\n",
      " |      \n",
      " |      The square root lasso uses the following keyword arguments:\n",
      " |      \n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      The cvxopt module is required to estimate model using the square root\n",
      " |      lasso.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
      " |         generalized linear models via coordinate descent.  Journal of\n",
      " |         Statistical Software 33(1), 1-22 Feb 2010.\n",
      " |      \n",
      " |      .. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso:\n",
      " |         pivotal recovery of sparse signals via conic programming.\n",
      " |         Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf\n",
      " |  \n",
      " |  hessian(self, params, scale=None)\n",
      " |      Evaluate the Hessian function at a given point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the Hessian is computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The Hessian matrix.\n",
      " |  \n",
      " |  hessian_factor(self, params, scale=None, observed=True)\n",
      " |      Calculate the weights for the Hessian.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The parameter at which Hessian is evaluated.\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is returned. If false then the\n",
      " |          expected information matrix is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          A 1d weight vector used in the calculation of the Hessian.\n",
      " |          The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
      " |  \n",
      " |  loglike(self, params, scale=None)\n",
      " |      The likelihood function for the OLS model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The coefficients with which to estimate the log-likelihood.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The likelihood function evaluated at params.\n",
      " |  \n",
      " |  score(self, params, scale=None)\n",
      " |      Evaluate the score function at a given point.\n",
      " |      \n",
      " |      The score corresponds to the profile (concentrated)\n",
      " |      log-likelihood in which the scale parameter has been profiled\n",
      " |      out.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameter vector at which the score function is\n",
      " |          computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray\n",
      " |          The score vector.\n",
      " |  \n",
      " |  whiten(self, x)\n",
      " |      OLS model whitener does nothing.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          Data to be whitened.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          The input array unmodified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      OLS : Fit a linear model using Ordinary Least Squares.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RegressionModel:\n",
      " |  \n",
      " |  fit(self, method='pinv', cov_type='nonrobust', cov_kwds=None, use_t=None, **kwargs)\n",
      " |      Full fit of the model.\n",
      " |      \n",
      " |      The results include an estimate of covariance matrix, (whitened)\n",
      " |      residuals and an estimate of scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str, optional\n",
      " |          Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
      " |          to solve the least squares problem. \"qr\" uses the QR\n",
      " |          factorization.\n",
      " |      cov_type : str, optional\n",
      " |          See `regression.linear_model.RegressionResults` for a description\n",
      " |          of the available covariance estimators.\n",
      " |      cov_kwds : list or None, optional\n",
      " |          See `linear_model.RegressionResults.get_robustcov_results` for a\n",
      " |          description required keywords for alternative covariance\n",
      " |          estimators.\n",
      " |      use_t : bool, optional\n",
      " |          Flag indicating to use the Student's t distribution when computing\n",
      " |          p-values.  Default behavior depends on cov_type. See\n",
      " |          `linear_model.RegressionResults.get_robustcov_results` for\n",
      " |          implementation details.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments that contain information used when\n",
      " |          constructing a model using the formula interface.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      RegressionResults\n",
      " |          The model estimation results.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      RegressionResults\n",
      " |          The results container.\n",
      " |      RegressionResults.get_robustcov_results\n",
      " |          A method to change the covariance estimator used when fitting the\n",
      " |          model.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The fit method uses the pseudoinverse of the design/exogenous variables\n",
      " |      to solve the least squares minimization.\n",
      " |  \n",
      " |  get_distribution(self, params, scale, exog=None, dist_class=None)\n",
      " |      Construct a random number generator for the predictive distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The model parameters (regression coefficients).\n",
      " |      scale : scalar\n",
      " |          The variance parameter.\n",
      " |      exog : array_like\n",
      " |          The predictor variable matrix.\n",
      " |      dist_class : class\n",
      " |          A random number generator class.  Must take 'loc' and 'scale'\n",
      " |          as arguments and return a random number generator implementing\n",
      " |          an ``rvs`` method for simulating random values. Defaults to normal.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      gen\n",
      " |          Frozen random number generator object with mean and variance\n",
      " |          determined by the fitted linear model.  Use the ``rvs`` method\n",
      " |          to generate random values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to the behavior of ``scipy.stats.distributions objects``,\n",
      " |      the returned random number generator must be called with\n",
      " |      ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
      " |      the data set used to fit the model.  If any other value is\n",
      " |      used for ``n``, misleading results will be produced.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize model components.\n",
      " |  \n",
      " |  predict(self, params, exog=None)\n",
      " |      Return linear predicted values from a design matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Parameters of a linear model.\n",
      " |      exog : array_like, optional\n",
      " |          Design / exogenous data. Model exog is used if None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array_like\n",
      " |          An array of fitted values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the model has not yet been fit, params is not optional.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RegressionModel:\n",
      " |  \n",
      " |  df_model\n",
      " |      The model degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the rank of the regressor matrix minus 1 if a\n",
      " |      constant is included.\n",
      " |  \n",
      " |  df_resid\n",
      " |      The residual degree of freedom.\n",
      " |      \n",
      " |      The dof is defined as the number of observations minus the rank of\n",
      " |      the regressor matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0336b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_formula in module statsmodels.base.model:\n",
      "\n",
      "from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) method of builtins.type instance\n",
      "    Create a Model from a formula and dataframe.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    formula : str or generic Formula object\n",
      "        The formula specifying the model.\n",
      "    data : array_like\n",
      "        The data for the model. See Notes.\n",
      "    subset : array_like\n",
      "        An array-like object of booleans, integers, or index values that\n",
      "        indicate the subset of df to use in the model. Assumes df is a\n",
      "        `pandas.DataFrame`.\n",
      "    drop_cols : array_like\n",
      "        Columns to drop from the design matrix.  Cannot be used to\n",
      "        drop terms involving categoricals.\n",
      "    *args\n",
      "        Additional positional argument that are passed to the model.\n",
      "    **kwargs\n",
      "        These are passed to the model with one exception. The\n",
      "        ``eval_env`` keyword is passed to patsy. It can be either a\n",
      "        :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      "        indicating the depth of the namespace to use. For example, the\n",
      "        default ``eval_env=0`` uses the calling namespace. If you wish\n",
      "        to use a \"clean\" environment set ``eval_env=-1``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    model\n",
      "        The model instance.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    data must define __getitem__ with the keys in the formula terms\n",
      "    args and kwargs are passed on to the model instantiation. E.g.,\n",
      "    a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
